{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка натюненной модели"
      ],
      "metadata": {
        "id": "arwGHm1jFgzO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предварительно необходимо загрузить модель в папку. Содержимое папки должно быть приблизительно таким:\n",
        "```txt\n",
        "adapter_config.json        README.md                tokenizer.json\n",
        "adapter_model.safetensors  special_tokens_map.json  tokenizer.model\n",
        "chat_template.jinja        tokenizer_config.json\n",
        "```"
      ],
      "metadata": {
        "id": "lnghNSh9Fl0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Необходимо получить токен HF ([ссылка](https://huggingface.co/settings/tokens))\n",
        "\n",
        "Загрузить переменную окружения с именем `HF_TOKEN` во вкладке Secrets. Вставить в эту перемееную ключ"
      ],
      "metadata": {
        "id": "-i54KSmWIvYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Код загрузки модели"
      ],
      "metadata": {
        "id": "MGY4F4ixJPuR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZ-T-7fDo_2Z",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -U huggingface_hub\n",
        "!pip install \"huggingface-hub>=0.34.0,<1.0\"\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import upload_folder\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from huggingface_hub import notebook_login, create_repo"
      ],
      "metadata": {
        "id": "BWzMdPzHINx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Переменные"
      ],
      "metadata": {
        "id": "8QKK4rUqKlGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Если модель загружана на google disk. Пусть к папке остается таким же."
      ],
      "metadata": {
        "id": "uIX4-k4X21Tq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "USE_GOOGLE_DISK = True"
      ],
      "metadata": {
        "id": "hUcLq0hG2w6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HF_TOKEN_ENV_VAR = \"HF_TOKEN\" # Имя переменной окружения\n",
        "\n",
        "BASE_MODEL_PATH = \"путь_где+хранится_модель\"\n",
        "MODEL_PATH = f\"./{BASE_MODEL_PATH}\" if not USE_GOOGLE_DISK else f\"/content/drive/MyDrive/{BASE_MODEL_PATH}\"\n",
        "\n",
        "HF_USERNAME = \"имя_на_Hugging_face\"\n",
        "MODEL_NAME = \"название_модели\"\n",
        "REPO_ID = f\"{HF_USERNAME}/{MODEL_NAME}\"\n",
        "\n",
        "IS_PRIVATE=False"
      ],
      "metadata": {
        "id": "5YLp3Uw8JTsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Загрузка модели"
      ],
      "metadata": {
        "id": "yLfw5TITKiow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_model_to_hf():\n",
        "    token = userdata.get(HF_TOKEN_ENV_VAR)\n",
        "    if not token:\n",
        "        raise EnvironmentError(f\"Переменная окружения {HF_TOKEN_ENV_VAR} не установлена.\")\n",
        "    create_repo(repo_id=REPO_ID, private=IS_PRIVATE, exist_ok=True)\n",
        "    upload_folder(\n",
        "        folder_path=MODEL_PATH,\n",
        "        repo_id=REPO_ID,\n",
        "        repo_type=\"model\",\n",
        "    )\n",
        "    print(f\"Модель успешно загрузилась {REPO_ID}\")\n",
        "\n",
        "upload_model_to_hf()"
      ],
      "metadata": {
        "id": "xvaiEL9xKpYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Тестирование модели - загрузка отправленной модели из HF"
      ],
      "metadata": {
        "id": "uI_NUbndK1O7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth trl peft accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "_R0skpc51054"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel"
      ],
      "metadata": {
        "id": "I1buDUZh14Ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_and_tokenizer():\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=REPO_ID,\n",
        "        max_seq_length=512,\n",
        "        dtype=None,\n",
        "        load_in_4bit=True,\n",
        "        force_download=True\n",
        "    )\n",
        "    return model, tokenizer\n",
        "\n",
        "model, tokenizer = load_model_and_tokenizer()"
      ],
      "metadata": {
        "id": "1UyZwrrQLJUt",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Сгенерируй конфликтную ситуацию\""
      ],
      "metadata": {
        "id": "wUpUC_5lK_89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_uploaded_model():\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "    print(f\"Входной запрос: {input_text}\\n\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"Ответ модели: {generated_text[len(input_text):]}\\n\")\n",
        "\n",
        "test_uploaded_model()"
      ],
      "metadata": {
        "id": "9glEtWtJK8fv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}