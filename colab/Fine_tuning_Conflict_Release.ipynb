{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eESUq_r5cSuX"
      },
      "source": [
        "# Установка зависимостей и библиотек"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cL3byIHfWEVx"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y unsloth peft\n",
        "!pip install unsloth trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DYyZ1Ip_igR2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import json\n",
        "import torch\n",
        "import textwrap\n",
        "from datasets import load_dataset, Dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C5iCCjRclYH"
      },
      "source": [
        "# Проверка на CUDA и GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "oOTWElUCWk_v"
      },
      "outputs": [],
      "source": [
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uIcHOo6iZWn"
      },
      "source": [
        "# Объявление переменных  и функций"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ShT075IqidfM"
      },
      "outputs": [],
      "source": [
        "BORDER_LEN = 150\n",
        "\n",
        "DATASET_FILENAME = \"dataset_full.json\"\n",
        "\n",
        "ARCHIVE_NAME = \"conflict_model_export\"\n",
        "\n",
        "MODEL_NAME = \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\"\n",
        "\n",
        "MAX_SEQ_LENGTH = 2024\n",
        "\n",
        "SEED = 1025\n",
        "\n",
        "CLEAR_FOLDER = False\n",
        "\n",
        "output_dir = \"./fine_tuned_model_conflict_export\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88FRzst37rF9"
      },
      "outputs": [],
      "source": [
        "# Отдельная загрузка модели\n",
        "original_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    force_download=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vMOkbu377zc-"
      },
      "outputs": [],
      "source": [
        "# Загрузка LoRA - low rank adaptation\n",
        "lora_model = FastLanguageModel.get_peft_model(\n",
        "    original_model,\n",
        "    r=256,  # LoRA rank - выше = больше емкость, больше память\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha=512,  # LoRA scaling factor (обычно 2x rank)\n",
        "    lora_dropout=0.1,  # Регуляризация для предотвращения переобучения\n",
        "    bias=\"lora_only\",  # Обучение смещений только для LoRA слоёв\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Оптимизированная версия Unsloth\n",
        "    random_state=SEED,\n",
        "    use_rslora=True,  # Rank stabilized LoRA для стабильности\n",
        "    loftq_config=None,  # LoftQ конфигурация\n",
        "    # init_lora_weights=\"loftq\"  # Инициализация LoftQ для лучшей точности\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0T3uSfazsqR5"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Иммитация чата.\n",
        "\n",
        "История чата - массив из след объектов:\n",
        "{\n",
        "  \"role\": \"роль\"\n",
        "  \"content\" : \"сообщение\"\n",
        "}\n",
        "\n",
        "Виды ролей:\n",
        "system - системный промпт. Указывает модели, кто и что она делает.\n",
        "user - сообщение отпоьзователя\n",
        "assistant - сообщение от модели\n",
        "\n",
        "Стратегия заполнения сообщениями (обязательные правила):\n",
        "1. Единственный системный промпт - system\n",
        "2. Пользователь - user\n",
        "3. Модель - assistant\n",
        "\n",
        "На каждый user должен быть свой assistant\n",
        "\n",
        "В конце блока остается одиночное сообщение от user - наш ключивой запрос\n",
        "\"\"\"\n",
        "\n",
        "MESSAGES = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"напиши мне конфликт между братом и сестрой\"\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caJN5BQZi5YL"
      },
      "outputs": [],
      "source": [
        "def generate_response(_model, _tokenizer):\n",
        "  global MESSAGES\n",
        "\n",
        "  FastLanguageModel.for_inference(_model)\n",
        "\n",
        "  # Input like chat\n",
        "  inputs = _tokenizer.apply_chat_template(\n",
        "      MESSAGES,\n",
        "      tokenize=True,\n",
        "      add_generation_prompt=True,\n",
        "      return_tensors=\"pt\",\n",
        "  ).to(\"cuda\")\n",
        "\n",
        "  # Generate response\n",
        "  outputs = _model.generate(\n",
        "      input_ids=inputs,\n",
        "      max_new_tokens=512,\n",
        "      use_cache=True,\n",
        "      temperature=0.7,\n",
        "      do_sample=True,\n",
        "      top_p=0.9,\n",
        "  )\n",
        "\n",
        "  response = _tokenizer.batch_decode(outputs)[0]\n",
        "\n",
        "  return response\n",
        "\n",
        "def format_prompt(example):\n",
        "  return f\"### Input: {example['input']}\\n### Output: {json.dumps(example['output'], ensure_ascii=False)}<|endoftext|>\"\n",
        "\n",
        "def format_model_output(response):\n",
        "  return response.split('<|assistant|>')[-1].split('<|end|>')[0].strip()\n",
        "\n",
        "def format_text_for_window(text, max_len=BORDER_LEN):\n",
        "    lines = text.split('\\n')\n",
        "    formatted_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        if len(line) <= max_len:\n",
        "            formatted_lines.append(line)\n",
        "        else:\n",
        "            wrapped = textwrap.fill(line, width=max_len, break_long_words=True, break_on_hyphens=True)\n",
        "            formatted_lines.extend(wrapped.split('\\n'))\n",
        "\n",
        "    return '\\n'.join(formatted_lines)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_s61vKscbDh"
      },
      "source": [
        "# Загрузка датасета и адаптация под нужный формат\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FIdADxFWXToO"
      },
      "outputs": [],
      "source": [
        "file = json.load(open(DATASET_FILENAME, \"r\"))\n",
        "formatted_data = [format_prompt(item) for item in file]\n",
        "\n",
        "DATASET = Dataset.from_dict({\"text\": formatted_data})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "30atuewF-60i"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*BORDER_LEN)\n",
        "print(\"ТЕКСТ ЗАПИСИ ИЗ ИСХОДНОГО ДАТАСЕТА:\")\n",
        "print(\"=\"*BORDER_LEN)\n",
        "print(format_text_for_window(json.dumps(file[0], indent=4, ensure_ascii=False)))\n",
        "print(\"=\"*BORDER_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OQ1z83YH1ixO"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*BORDER_LEN)\n",
        "print(\"ОТФОРМАТИРОВАННЫЙ ДЛЯ ТРЕНЕРОВКИ ДАТАСЕТ\")\n",
        "print(\"=\"*BORDER_LEN)\n",
        "print(format_text_for_window(formatted_data[0]))\n",
        "print(\"=\"*BORDER_LEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHrxksjjcs8l"
      },
      "source": [
        "# Загрузка модели и тренера из Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm8booC8XliQ"
      },
      "outputs": [],
      "source": [
        "# Training arguments optimized for Unsloth\n",
        "trainer = SFTTrainer(\n",
        "    model=lora_model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=DATASET,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    dataset_num_proc=20,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=8,\n",
        "        gradient_accumulation_steps=32,  # Effective batch size = 8\n",
        "        warmup_steps=20,\n",
        "        num_train_epochs=10,\n",
        "        learning_rate=1e-5,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=25,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        seed=SEED,\n",
        "        output_dir=\"outputs\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=2,\n",
        "        dataloader_pin_memory=False,\n",
        "        report_to=\"none\", # Disable Weights & Biases logging\n",
        "    ),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSDKVkKbgEAh"
      },
      "source": [
        "# Тестирование"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZmXvkRr3A9J"
      },
      "source": [
        "При тестировании модели до/после необходимо: провести тест ДО тренеровки, НАТРЕНЕРОВАТЬ модель, ПОСЛЕ получить ответ натренерованной модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOhzTldldM5s"
      },
      "source": [
        "## Тестирование оригинальной модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "al7BoFU4Z7Pw"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*BORDER_LEN)\n",
        "print(\"ОТВЕТ МОДЕЛИ:\")\n",
        "print(\"=\"*BORDER_LEN)\n",
        "print(format_model_output(generate_response(lora_model, tokenizer)))\n",
        "print(\"=\"*BORDER_LEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfh2liI1dS-m"
      },
      "source": [
        "## Тренеровка модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uZrtr0c4XmTE"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnxhDosAdW6A"
      },
      "source": [
        "## Тестирование натренерованной модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vBGnx7DXuN9"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*BORDER_LEN)\n",
        "print(\"ОТВЕТ НАТРЕНЕРОВАННОЙ МОДЕЛИ:\")\n",
        "print(\"=\"*BORDER_LEN)\n",
        "print(format_text_for_window(format_model_output(generate_response(lora_model, tokenizer))))\n",
        "print(\"=\"*BORDER_LEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhEiJ2kvflbx"
      },
      "source": [
        "# Статистика"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLd-et1ofoGy"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*BORDER_LEN)\n",
        "print(\"ИТОГОВАЯ СТАТИСТИКА ОБУЧЕНИЯ\")\n",
        "print(\"=\"*BORDER_LEN)\n",
        "\n",
        "print(f\"Общее количество шагов: {trainer_stats.global_step}\")\n",
        "print(f\"Общее время обучения: {trainer_stats.metrics.get('train_runtime', 'N/A')} секунд\")\n",
        "print(f\"Средняя скорость обучения: {trainer_stats.metrics.get('train_samples_per_second', 'N/A')} образцов/сек\")\n",
        "print(f\"Количество обработанных образцов: {trainer_stats.metrics.get('train_samples', 'N/A')}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*BORDER_LEN)\n",
        "print(\"ПОТЕРИ (LOSS) ПО ЭТАПАМ:\")\n",
        "print(\"-\"*BORDER_LEN)\n",
        "\n",
        "if hasattr(trainer, 'state') and trainer.state.log_history:\n",
        "    for i, log_entry in enumerate(trainer.state.log_history[-10:], 1):\n",
        "        if 'loss' in log_entry:\n",
        "            step = log_entry.get('step', 'N/A')\n",
        "            loss = log_entry.get('loss', 'N/A')\n",
        "            learning_rate = log_entry.get('learning_rate', 'N/A')\n",
        "            print(f\"Шаг {step}: Loss = {loss:.4f}, LR = {learning_rate:.2e}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*BORDER_LEN)\n",
        "print(\"ФИНАЛЬНЫЕ МЕТРИКИ:\")\n",
        "print(\"-\"*BORDER_LEN)\n",
        "\n",
        "for key, value in trainer_stats.metrics.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*BORDER_LEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3Xxif2ibuiZ"
      },
      "source": [
        "# Экспорт"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQbvmk1Cbt8l"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(lora_model)\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "try:\n",
        "    lora_model.save_pretrained(output_dir)\n",
        "    tokenizer.save_pretrained(output_dir)\n",
        "    print(\"=\"*BORDER_LEN)\n",
        "    print(f\"Файлы модели успешно сохранены в: {output_dir}\")\n",
        "    print(\"=\"*BORDER_LEN)\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при сохранении модели: {e}\")\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "zip_filename = f\"{ARCHIVE_NAME}_{timestamp}.zip\"\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(output_dir):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                archive_path = os.path.relpath(file_path, output_dir)\n",
        "                zipf.write(file_path, archive_path)\n",
        "    print(f\"Модель успешно экспортирована в ZIP-архив: {zip_filename}\")\n",
        "    print(\"=\"*BORDER_LEN)\n",
        "except Exception as e:\n",
        "    print(\"=\"*BORDER_LEN)\n",
        "    print(f\"Ошибка при создании ZIP-архива: {e}\")\n",
        "    print(\"=\"*BORDER_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if CLEAR_FOLDER:\n",
        "  try:\n",
        "      shutil.rmtree(output_dir)\n",
        "      print(f\"Временная директория {output_dir} удалена после архивации.\")\n",
        "      print(\"=\"*BORDER_LEN)\n",
        "  except Exception as e:\n",
        "      print(f\"Не удалось удалить временную директорию {output_dir}: {e}\")\n",
        "      print(\"=\"*BORDER_LEN)"
      ],
      "metadata": {
        "id": "dxjgLGqDLbQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка натренерованной модели"
      ],
      "metadata": {
        "id": "ND5ltLgeAuXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model_dir = output_dir\n",
        "\n",
        "try:\n",
        "    fine_tuned_lora_model, fine_tuned_tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=saved_model_dir,\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        dtype=None,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "    print(f\"Модель LoRA успешно загружена из: {saved_model_dir}\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при загрузке модели LoRA: {e}\")\n",
        "\n",
        "try:\n",
        "    fine_tuned_full_model = fine_tuned_lora_model.merge_and_unload()\n",
        "    print(\"LoRA адаптация успешно применена. Полная модифицированная модель готова.\")\n",
        "except Exception as e:\n",
        "    print(f\"Ошибка при применении LoRA адаптации: {e}\")\n",
        "    fine_tuned_full_model = fine_tuned_lora_model\n",
        "    print(\"Продолжаем с моделью LoRA.\")\n",
        "\n",
        "FastLanguageModel.for_inference(fine_tuned_full_model if fine_tuned_full_model is not None else fine_tuned_lora_model)"
      ],
      "metadata": {
        "id": "b9voiehhAt0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*BORDER_LEN)\n",
        "print(\"ОТВЕТ НАТРЕНЕРОВАННОЙ МОДЕЛИ:\")\n",
        "print(\"=\"*BORDER_LEN)\n",
        "print(format_text_for_window(format_model_output(generate_response(fine_tuned_lora_model, fine_tuned_tokenizer))))\n",
        "print(\"=\"*BORDER_LEN)"
      ],
      "metadata": {
        "id": "uZNEVQFwDGce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Сохранение модели на google disk"
      ],
      "metadata": {
        "id": "BzVM95xL8xpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GaxUQRGL9_3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_DISK_MODEL_FOLDER = \"conflict_mistral_model\"\n",
        "LOCAL_MODEL_PATH = output_dir if output_dir is not None else './fine_tuned_model_conflict_export'"
      ],
      "metadata": {
        "id": "lyegvhQK_9z2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_folder_path = f'/content/drive/MyDrive/{conflict_mistral_model}'\n",
        "os.makedirs(model_folder_path, exist_ok=True)\n",
        "\n",
        "if os.path.exists(LOCAL_MODEL_PATH):\n",
        "    for item in os.listdir(LOCAL_MODEL_PATH):\n",
        "        source_item = os.path.join(LOCAL_MODEL_PATH, item)\n",
        "        dest_item = os.path.join(model_folder_path, item)\n",
        "        if os.path.isdir(source_item):\n",
        "            shutil.copytree(source_item, dest_item, dirs_exist_ok=True)\n",
        "        else:\n",
        "            shutil.copy2(source_item, dest_item)\n",
        "    print(f\"Модель сохранена {model_folder_path}\")\n",
        "else:\n",
        "    print(f\"Папка с моделью {LOCAL_MODEL_PATH} не найдена.\")"
      ],
      "metadata": {
        "id": "4P934G5m-8Pq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}